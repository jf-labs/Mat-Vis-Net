{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38f05134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\j0sep\\Mat-Vis-Net\n",
      "CSV path: C:\\Users\\j0sep\\Mat-Vis-Net\\data\\san_leandro_products.csv\n",
      "Images dir: C:\\Users\\j0sep\\Mat-Vis-Net\\data\\images\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# Assume you start Jupyter from the repo root.\n",
    "# If you start it from notebooks/, change \".\" to \"..\".\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "CSV_PATH = DATA_DIR / \"san_leandro_products.csv\"\n",
    "EMB_PATH = DATA_DIR / \"image_embs.npy\"\n",
    "FILTERED_CSV_PATH = DATA_DIR / \"san_leandro_products_with_embs.csv\"\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"CSV path:\", CSV_PATH)\n",
    "print(\"Images dir:\", IMAGES_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93136c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with images: 2985\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>name</th>\n",
       "      <th>category_slug</th>\n",
       "      <th>product_url</th>\n",
       "      <th>image_url</th>\n",
       "      <th>image_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100997154</td>\n",
       "      <td>Captains Runs Waterproof Laminate Plank</td>\n",
       "      <td>/10mm-and-above-laminate</td>\n",
       "      <td>https://www.flooranddecor.com/aquaguard-perfor...</td>\n",
       "      <td>https://i8.amplience.net/i/flooranddecor/10099...</td>\n",
       "      <td>100997154.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100992742</td>\n",
       "      <td>East Bay Breeze Waterproof Laminate Plank</td>\n",
       "      <td>/10mm-and-above-laminate</td>\n",
       "      <td>https://www.flooranddecor.com/aquaguard-perfor...</td>\n",
       "      <td>https://i8.amplience.net/i/flooranddecor/10099...</td>\n",
       "      <td>100992742.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100997105</td>\n",
       "      <td>Cocoa Waterproof Laminate Plank</td>\n",
       "      <td>/10mm-and-above-laminate</td>\n",
       "      <td>https://www.flooranddecor.com/aquaguard-perfor...</td>\n",
       "      <td>https://i8.amplience.net/i/flooranddecor/10099...</td>\n",
       "      <td>100997105.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101006005</td>\n",
       "      <td>Mountain Valley Waterproof Laminate Plank</td>\n",
       "      <td>/10mm-and-above-laminate</td>\n",
       "      <td>https://www.flooranddecor.com/aquaguard-perfor...</td>\n",
       "      <td>https://i8.amplience.net/i/flooranddecor/10100...</td>\n",
       "      <td>101006005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101235778</td>\n",
       "      <td>Easton Hickory Waterproof Laminate</td>\n",
       "      <td>/10mm-and-above-laminate</td>\n",
       "      <td>https://www.flooranddecor.com/hydroshield-plus...</td>\n",
       "      <td>https://i8.amplience.net/i/flooranddecor/10123...</td>\n",
       "      <td>101235778.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sku                                       name  \\\n",
       "0  100997154    Captains Runs Waterproof Laminate Plank   \n",
       "1  100992742  East Bay Breeze Waterproof Laminate Plank   \n",
       "2  100997105            Cocoa Waterproof Laminate Plank   \n",
       "3  101006005  Mountain Valley Waterproof Laminate Plank   \n",
       "4  101235778         Easton Hickory Waterproof Laminate   \n",
       "\n",
       "              category_slug  \\\n",
       "0  /10mm-and-above-laminate   \n",
       "1  /10mm-and-above-laminate   \n",
       "2  /10mm-and-above-laminate   \n",
       "3  /10mm-and-above-laminate   \n",
       "4  /10mm-and-above-laminate   \n",
       "\n",
       "                                         product_url  \\\n",
       "0  https://www.flooranddecor.com/aquaguard-perfor...   \n",
       "1  https://www.flooranddecor.com/aquaguard-perfor...   \n",
       "2  https://www.flooranddecor.com/aquaguard-perfor...   \n",
       "3  https://www.flooranddecor.com/aquaguard-perfor...   \n",
       "4  https://www.flooranddecor.com/hydroshield-plus...   \n",
       "\n",
       "                                           image_url image_filename  \n",
       "0  https://i8.amplience.net/i/flooranddecor/10099...  100997154.jpg  \n",
       "1  https://i8.amplience.net/i/flooranddecor/10099...  100992742.jpg  \n",
       "2  https://i8.amplience.net/i/flooranddecor/10099...  100997105.jpg  \n",
       "3  https://i8.amplience.net/i/flooranddecor/10100...  101006005.jpg  \n",
       "4  https://i8.amplience.net/i/flooranddecor/10123...  101235778.jpg  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Adjust the column name if yours is different\n",
    "IMAGE_COL = \"image_filename\"\n",
    "\n",
    "if IMAGE_COL not in df.columns:\n",
    "    raise ValueError(f\"{IMAGE_COL!r} column not found in CSV. Check your column names.\")\n",
    "\n",
    "# Keep only rows that actually have an image file\n",
    "df = df[df[IMAGE_COL].notna() & (df[IMAGE_COL] != \"\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"Rows with images:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4640442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23c87247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(path: Path) -> np.ndarray:\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "\n",
    "    # Normalize and flatten to 1D numpy\n",
    "    emb = outputs[0]\n",
    "    emb = emb / emb.norm(p=2)\n",
    "    return emb.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1369af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_list = []\n",
    "kept_rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    img_name = row[IMAGE_COL]\n",
    "    img_path = IMAGES_DIR / img_name\n",
    "\n",
    "    if not img_path.exists():\n",
    "        # Skip rows with missing files\n",
    "        continue\n",
    "\n",
    "    emb = embed_image(img_path)\n",
    "    emb_list.append(emb)\n",
    "    kept_rows.append(row)\n",
    "\n",
    "len(emb_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7874611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows with images: 2985\n",
      "Using material source column: category_slug\n",
      "material_group\n",
      "other              2561\n",
      "wood                186\n",
      "vinyl                90\n",
      "laminate             70\n",
      "engineered_wood      52\n",
      "porcelain            15\n",
      "solid_wood            8\n",
      "ceramic               3\n",
      "Name: count, dtype: int64\n",
      "device: cpu\n",
      "missing image files: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f537223dcdc4176bc2b5425dc5f7654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- paths ---\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "CSV_PATH = PROJECT_ROOT / \"data\" / \"san_leandro_products.csv\"\n",
    "IMAGES_DIR = PROJECT_ROOT / \"images\"\n",
    "\n",
    "# --- load metadata ---\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df[df[\"image_filename\"].notna() & (df[\"image_filename\"] != \"\")].reset_index(drop=True)\n",
    "print(\"rows with images:\", len(df))\n",
    "\n",
    "# ---------- STEP 1: material grouping ----------\n",
    "MATERIAL_SOURCE_COLS = [\"body\", \"material\", \"category_slug\"]\n",
    "\n",
    "def infer_material_source_col(df):\n",
    "    for col in MATERIAL_SOURCE_COLS:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "MATERIAL_COL = infer_material_source_col(df)\n",
    "print(\"Using material source column:\", MATERIAL_COL)\n",
    "\n",
    "def normalize_material(text: str) -> str:\n",
    "    t = str(text).lower()\n",
    "\n",
    "    # --- Tile families ---\n",
    "    if \"porcelain\" in t:\n",
    "        if \"wood\" in t:\n",
    "            return \"wood_look_porcelain\"\n",
    "        return \"porcelain\"\n",
    "    if \"ceramic\" in t:\n",
    "        return \"ceramic\"\n",
    "\n",
    "    # --- Wood / wood-like families ---\n",
    "    if \"laminate\" in t:\n",
    "        return \"laminate\"\n",
    "    if \"vinyl\" in t or \"lvp\" in t or \"lvt\" in t:\n",
    "        return \"vinyl\"\n",
    "    if \"engineered\" in t:\n",
    "        return \"engineered_wood\"\n",
    "    if \"solid\" in t and (\"hardwood\" in t or \"wood\" in t):\n",
    "        return \"solid_wood\"\n",
    "    if \"hardwood\" in t or \"wood\" in t:\n",
    "        return \"wood\"\n",
    "\n",
    "    return \"other\"\n",
    "\n",
    "if MATERIAL_COL is not None:\n",
    "    df[\"material_group\"] = df[MATERIAL_COL].apply(normalize_material)\n",
    "else:\n",
    "    df[\"material_group\"] = \"other\"\n",
    "\n",
    "print(df[\"material_group\"].value_counts())\n",
    "\n",
    "# ---------- CLIP model ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# first time this line runs, it may download the processor files\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- helpers ---\n",
    "def load_image(path: Path):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def embed_images(image_paths, batch_size=16):\n",
    "    all_embs = []\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = [load_image(p) for p in batch_paths]\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.get_image_features(**inputs)  # [B, D]\n",
    "\n",
    "        # normalize\n",
    "        embs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n",
    "        all_embs.append(embs.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "# --- build list of image paths ---\n",
    "image_paths = [IMAGES_DIR / fname for fname in df[\"image_filename\"].tolist()]\n",
    "missing = [p for p in image_paths if not p.exists()]\n",
    "print(\"missing image files:\", len(missing))\n",
    "\n",
    "# --- actually compute embeddings ---\n",
    "image_embs = embed_images(image_paths, batch_size=16)\n",
    "print(\"image_embs shape:\", image_embs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0559a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# NearestNeighbors with cosine distance (1 - cosine similarity)\n",
    "nn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
    "nn.fit(image_embs)\n",
    "\n",
    "print(\"Index built over\", image_embs.shape[0], \"products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b33aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_by_index(query_idx, top_k=5):\n",
    "    \"\"\"\n",
    "    Return the top_k nearest neighbors for a given product index,\n",
    "    excluding the product itself, deduping by image, and enforcing\n",
    "    material-aware filtering.\n",
    "\n",
    "    - Never return the exact same SKU as the query.\n",
    "    - Never show the same image twice (even if SKU differs).\n",
    "    - If the query is any kind of wood/laminate/vinyl/etc, only return\n",
    "      the *same* material_group (so wood won't match laminate/vinyl/\n",
    "      solid/engineered and vice versa).\n",
    "    - Tile vs non-tile is also separated (porcelain/ceramic vs wood stuff).\n",
    "    \"\"\"\n",
    "    # Embedding for the query product\n",
    "    query_emb = image_embs[query_idx].reshape(1, -1)\n",
    "    query_row = df.iloc[query_idx]\n",
    "    query_sku = query_row[\"sku\"]\n",
    "    query_group = query_row.get(\"material_group\", \"other\")\n",
    "\n",
    "    # Ask for some extra neighbors to survive filtering/dedup\n",
    "    n_neighbors = min(top_k + 20, len(df))\n",
    "    distances, indices = nn.kneighbors(query_emb, n_neighbors=n_neighbors)\n",
    "    distances = distances[0]\n",
    "    indices = indices[0]\n",
    "\n",
    "    results = []\n",
    "    seen_images = set()\n",
    "\n",
    "    # Groups that we treat as wood-family, but we still\n",
    "    # don't mix them with each other unless group matches exactly.\n",
    "    wood_groups = {\"wood\", \"engineered_wood\", \"solid_wood\", \"laminate\", \"vinyl\"}\n",
    "\n",
    "    # Tile-like groups\n",
    "    tile_groups = {\"porcelain\", \"ceramic\", \"wood_look_porcelain\"}\n",
    "\n",
    "    for dist, idx in zip(distances, indices):\n",
    "        # Skip the exact same row\n",
    "        if idx == query_idx:\n",
    "            continue\n",
    "\n",
    "        row = df.iloc[idx]\n",
    "\n",
    "        # Skip same SKU as the query\n",
    "        if row[\"sku\"] == query_sku:\n",
    "            continue\n",
    "\n",
    "        img_key = row[\"image_filename\"]\n",
    "\n",
    "        # Skip duplicate images\n",
    "        if img_key in seen_images:\n",
    "            continue\n",
    "\n",
    "        candidate_group = row.get(\"material_group\", \"other\")\n",
    "\n",
    "        # ---------- MATERIAL FILTERING ----------\n",
    "\n",
    "        if query_group in wood_groups:\n",
    "            # If query is wood-like (wood, laminate, vinyl, engineeered, solid),\n",
    "            # require exact same material_group.\n",
    "            #\n",
    "            # This is where your rule kicks in:\n",
    "            # \"if it's wood, exclude laminate vs vinyl vs solid vs engineered\"\n",
    "            # because those are all separate groups.\n",
    "            if candidate_group != query_group:\n",
    "                continue\n",
    "        else:\n",
    "            # For non-wood queries, keep tile vs non-tile separated.\n",
    "            query_is_tile = query_group in tile_groups\n",
    "            candidate_is_tile = candidate_group in tile_groups\n",
    "\n",
    "            # Don't mix tile with non-tile\n",
    "            if query_is_tile != candidate_is_tile:\n",
    "                continue\n",
    "\n",
    "        # ---------- END MATERIAL FILTERING ----------\n",
    "\n",
    "        seen_images.add(img_key)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"rank\": len(results) + 1,\n",
    "                \"sku\": row[\"sku\"],\n",
    "                \"name\": row[\"name\"],\n",
    "                \"category\": row[\"category_slug\"],\n",
    "                \"material_group\": candidate_group,\n",
    "                \"distance\": float(dist),\n",
    "                \"image_path\": str(IMAGES_DIR / row[\"image_filename\"]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Pick a sample index (try different numbers later)\n",
    "query_idx = 0\n",
    "\n",
    "query_row = df.iloc[query_idx]\n",
    "query_img = load_image(IMAGES_DIR / query_row[\"image_filename\"])\n",
    "\n",
    "print(\"Query SKU:\", query_row[\"sku\"])\n",
    "print(\"Name:\", query_row[\"name\"])\n",
    "print(\"Category:\", query_row[\"category_slug\"])\n",
    "display(query_img)\n",
    "\n",
    "results = search_similar_by_index(query_idx, top_k=5)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    print(f\"Rank {r['rank']} | SKU {r['sku']} | dist={r['distance']:.4f}\")\n",
    "    display(load_image(Path(r[\"image_path\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 20  # example\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "labels = kmeans.fit_predict(image_embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b00d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBS_PATH = PROJECT_ROOT / \"data\" / \"image_embs.npy\"\n",
    "np.save(EMBS_PATH, image_embs)\n",
    "print(\"Saved embeddings to:\", EMBS_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
